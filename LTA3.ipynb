{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSwEo4XSjrvJ",
        "outputId": "551bf5a3-4b87-4ca6-dff4-37b38a963823"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original missing counts:\n",
            " age       177\n",
            "fare        0\n",
            "pclass      0\n",
            "parch       0\n",
            "sibsp       0\n",
            "dtype: int64\n",
            "\n",
            "After dropna - shape: (714, 5) missing:\n",
            " age       0\n",
            "fare      0\n",
            "pclass    0\n",
            "parch     0\n",
            "sibsp     0\n",
            "dtype: int64\n",
            "\n",
            "Mean imputed missing counts:\n",
            " age       0\n",
            "fare      0\n",
            "pclass    0\n",
            "parch     0\n",
            "sibsp     0\n",
            "dtype: int64\n",
            "Median imputed missing counts:\n",
            " age       0\n",
            "fare      0\n",
            "pclass    0\n",
            "parch     0\n",
            "sibsp     0\n",
            "dtype: int64\n",
            "\n",
            "KNN imputed missing counts:\n",
            " age       0\n",
            "fare      0\n",
            "pclass    0\n",
            "parch     0\n",
            "sibsp     0\n",
            "dtype: int64\n",
            "Iterative imputed missing counts:\n",
            " age       0\n",
            "fare      0\n",
            "pclass    0\n",
            "parch     0\n",
            "sibsp     0\n",
            "dtype: int64\n",
            "\n",
            "Age stats comparison:\n",
            "Original (drop missing): mean=29.70, median=28.00, std=14.53\n",
            "Mean: mean=29.70, median=29.70, std=13.00\n",
            "Median: mean=29.36, median=28.00, std=13.02\n",
            "KNN: mean=29.86, median=29.00, std=13.39\n",
            "Iterative: mean=29.29, median=27.63, std=13.61\n"
          ]
        }
      ],
      "source": [
        "#Q1\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.experimental import enable_iterative_imputer  # noqa: F401\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "\n",
        "titanic = sns.load_dataset(\"titanic\")\n",
        "\n",
        "\n",
        "cols = [\"age\", \"fare\", \"pclass\", \"parch\", \"sibsp\"]\n",
        "df = titanic[cols].copy()\n",
        "print(\"Original missing counts:\\n\", df.isna().sum())\n",
        "\n",
        "\n",
        "df_drop = df.dropna()\n",
        "print(\"\\nAfter dropna - shape:\", df_drop.shape, \"missing:\\n\", df_drop.isna().sum())\n",
        "\n",
        "\n",
        "df_mean = df.copy()\n",
        "df_median = df.copy()\n",
        "df_mean[\"age\"] = df_mean[\"age\"].fillna(df_mean[\"age\"].mean())\n",
        "df_median[\"age\"] = df_median[\"age\"].fillna(df_median[\"age\"].median())\n",
        "\n",
        "df_mean[\"fare\"] = df_mean[\"fare\"].fillna(df_mean[\"fare\"].mean())\n",
        "df_median[\"fare\"] = df_median[\"fare\"].fillna(df_median[\"fare\"].median())\n",
        "\n",
        "print(\"\\nMean imputed missing counts:\\n\", df_mean.isna().sum())\n",
        "print(\"Median imputed missing counts:\\n\", df_median.isna().sum())\n",
        "\n",
        "\n",
        "df_num = df.copy()\n",
        "knn = KNNImputer(n_neighbors=5)\n",
        "iter_imp = IterativeImputer(random_state=42)\n",
        "\n",
        "df_knn = pd.DataFrame(knn.fit_transform(df_num), columns=df_num.columns, index=df_num.index)\n",
        "df_iter = pd.DataFrame(iter_imp.fit_transform(df_num), columns=df_num.columns, index=df_num.index)\n",
        "\n",
        "print(\"\\nKNN imputed missing counts:\\n\", df_knn.isna().sum())\n",
        "print(\"Iterative imputed missing counts:\\n\", df_iter.isna().sum())\n",
        "\n",
        "\n",
        "def stats(name, s):\n",
        "    return f\"{name}: mean={s.mean():.2f}, median={s.median():.2f}, std={s.std():.2f}\"\n",
        "\n",
        "print(\"\\nAge stats comparison:\")\n",
        "print(stats(\"Original (drop missing)\", df_drop[\"age\"]))\n",
        "print(stats(\"Mean\", df_mean[\"age\"]))\n",
        "print(stats(\"Median\", df_median[\"age\"]))\n",
        "print(stats(\"KNN\", df_knn[\"age\"]))\n",
        "print(stats(\"Iterative\", df_iter[\"age\"]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q2\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "data = fetch_california_housing(as_frame=True)\n",
        "X = data.frame.drop(columns=[\"MedHouseVal\"])\n",
        "feature = \"AveRooms\"\n",
        "\n",
        "mm = MinMaxScaler()\n",
        "ss = StandardScaler()\n",
        "\n",
        "X_mm = pd.DataFrame(mm.fit_transform(X), columns=X.columns)\n",
        "X_ss = pd.DataFrame(ss.fit_transform(X), columns=X.columns)\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(14, 4), constrained_layout=True)\n",
        "axes[0].hist(X[feature], bins=40, color=\"gray\", edgecolor=\"black\")\n",
        "axes[0].set_title(f\"Original {feature}\")\n",
        "\n",
        "axes[1].hist(X_mm[feature], bins=40, color=\"skyblue\", edgecolor=\"black\")\n",
        "axes[1].set_title(f\"MinMaxScaled {feature}\")\n",
        "\n",
        "axes[2].hist(X_ss[feature], bins=40, color=\"salmon\", edgecolor=\"black\")\n",
        "axes[2].set_title(f\"StandardScaled {feature}\")\n",
        "\n",
        "for ax in axes:\n",
        "    ax.grid(alpha=0.3)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"Note: StandardScaler is generally more appropriate when features have outliers,\")\n",
        "print(\"because MinMaxScaler is highly sensitive to extreme values and can squash the bulk of the data.\")"
      ],
      "metadata": {
        "id": "72YODdxSj9Zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q3\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "titanic = sns.load_dataset(\"titanic\")\n",
        "cat = titanic[\"embarked\"].dropna().astype(str)  # C, Q, S\n",
        "\n",
        "\n",
        "le = LabelEncoder()\n",
        "labels = le.fit_transform(cat)\n",
        "\n",
        "\n",
        "ohe = OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n",
        "onehot = ohe.fit_transform(cat.to_frame())\n",
        "\n",
        "print(\"Original categories (first 5):\", cat.head().tolist())\n",
        "print(\"Label Encoded (first 5):\", labels[:5].tolist())\n",
        "print(\"One-Hot Encoded shape:\", onehot.shape, \"categories:\", ohe.categories_)\n",
        "\n",
        "print(\"\\nGuidance:\")\n",
        "print(\"- Use LabelEncoder when the model can naturally handle categorical codes or when there is a true ordinal relationship.\")\n",
        "print(\"- Use OneHotEncoder for nominal categories with no order (most tree/linear models), avoiding implicit ordinality.\")\n"
      ],
      "metadata": {
        "id": "W2lc62i5kFmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q4\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def minmax_normalize(x):\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    x_min, x_max = np.nanmin(x), np.nanmax(x)\n",
        "    if x_max == x_min:\n",
        "        return np.zeros_like(x)\n",
        "    return (x - x_min) / (x_max - x_min)\n",
        "\n",
        "iris = load_iris(as_frame=True)\n",
        "X = iris.data.copy()\n",
        "col = \"sepal length (cm)\"\n",
        "\n",
        "manual = minmax_normalize(X[col].values)\n",
        "scaler = MinMaxScaler()\n",
        "sk_scaled = scaler.fit_transform(X[[col]]).ravel()\n",
        "\n",
        "print(\"First 5 - Manual:\", np.round(manual[:5], 4))\n",
        "print(\"First 5 - Sklearn:\", np.round(sk_scaled[:5], 4))\n",
        "print(\"Max abs diff:\", np.max(np.abs(manual - sk_scaled)))"
      ],
      "metadata": {
        "id": "Dj1W4E-XkPv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q5\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "titanic = sns.load_dataset(\"titanic\")\n",
        "df = titanic[[\"age\", \"class\", \"sex\", \"fare\"]].copy()\n",
        "\n",
        "print(\"Missing before:\\n\", df.isna().sum())\n",
        "\n",
        "group_medians = df.groupby([\"class\", \"sex\"])[\"age\"].transform(\"median\")\n",
        "\n",
        "df[\"age_imputed\"] = df[\"age\"].fillna(group_medians)\n",
        "\n",
        "print(\"\\nMissing after (age_imputed):\\n\", df[[\"age\", \"age_imputed\"]].isna().sum())\n",
        "print(\"\\nSample rows:\\n\", df.head(10))"
      ],
      "metadata": {
        "id": "EaSIdwhAkVXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q6\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "data = fetch_california_housing(as_frame=True)\n",
        "y = data.frame[\"MedHouseVal\"].to_frame(name=\"value\")\n",
        "\n",
        "\n",
        "z = (y[\"value\"] - y[\"value\"].mean()) / y[\"value\"].std()\n",
        "z_outliers = y[np.abs(z) > 3]  # threshold 3\n",
        "\n",
        "\n",
        "Q1, Q3 = y[\"value\"].quantile(0.25), y[\"value\"].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
        "iqr_outliers = y[(y[\"value\"] < lower) | (y[\"value\"] > upper)]\n",
        "\n",
        "print(\"Z-score outliers:\", z_outliers.shape[0])\n",
        "print(\"IQR outliers:\", iqr_outliers.shape[0])\n",
        "print(\"Z-score thresholds: |z|>3\")\n",
        "print(f\"IQR thresholds: lower={lower:.3f}, upper={upper:.3f}\")"
      ],
      "metadata": {
        "id": "HnPLpC8ikbnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q7\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "titanic = sns.load_dataset(\"titanic\")\n",
        "df = titanic[[\"sibsp\", \"parch\", \"survived\"]].copy()\n",
        "\n",
        "df[\"FamilySize\"] = df[\"sibsp\"].fillna(0) + df[\"parch\"].fillna(0) + 1\n",
        "\n",
        "mm = MinMaxScaler()\n",
        "ss = StandardScaler()\n",
        "\n",
        "df[\"FamilySize_norm\"] = mm.fit_transform(df[[\"FamilySize\"]])\n",
        "df[\"FamilySize_std\"] = ss.fit_transform(df[[\"FamilySize\"]])\n",
        "\n",
        "print(df[[\"sibsp\", \"parch\", \"FamilySize\", \"FamilySize_norm\", \"FamilySize_std\"]].head(10))"
      ],
      "metadata": {
        "id": "t5ipFb6Xkfjh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}